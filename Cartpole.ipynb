{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simulates cartpole starting at x0 with action u -- ported from CS287 Matlab code\n",
    "'''\n",
    "\n",
    "def sim_cartpole(x0, u, dt):\n",
    "    \n",
    "    def dynamics(x, u):\n",
    "        mc = 10\n",
    "        mp = 1\n",
    "        l = 0.5\n",
    "        g = 9.81\n",
    "        T = 0.25\n",
    "        s = np.sin(x[1])\n",
    "        c = np.cos(x[1])\n",
    "        \n",
    "        xddot = (u + np.multiply(mp*s, l*np.power(x[3],2) + g*c))/(mc + mp*np.power(s,2))\n",
    "        tddot = (-u*c - np.multiply(np.multiply(mp*l*np.power(x[3],2), c),s) - \n",
    "                 np.multiply((mc+mp)*g,s)) / (l * (mc + np.multiply(mp, np.power(s,2))))\n",
    "        xdot = x[2:4]\n",
    "        xdot = np.append(xdot, xddot)\n",
    "        xdot = np.append(xdot, tddot)\n",
    "        \n",
    "        return xdot\n",
    "    \n",
    "    DT = 0.1\n",
    "    t = 0\n",
    "    while t < dt:\n",
    "        current_dt = min(DT, dt-t)\n",
    "        x0 = x0 + current_dt * dynamics(x0, u)\n",
    "        t = t + current_dt\n",
    "    \n",
    "    return x0\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Linearizes the dynamics of cartpole around a reference point for use in an LQR controler\n",
    "'''\n",
    "\n",
    "def linearize_cartpole(x_ref, u_ref, dt, eps):\n",
    "    A = np.zeros([4,4])\n",
    "\n",
    "    for i in range(4):\n",
    "        increment = np.zeros([4,])\n",
    "        increment[i] = eps\n",
    "        A[:,i] = (sim_cartpole(x_ref + increment, u_ref, dt) - \n",
    "                  sim_cartpole(x_ref, u_ref, dt)) / (eps)\n",
    "    \n",
    "    B = (sim_cartpole(x_ref, u_ref + eps, dt) - sim_cartpole(x_ref, u_ref, dt)) / (eps)\n",
    "    \n",
    "    c = x_ref\n",
    "    \n",
    "    return A, B, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Computes the LQR infinte horizon controller associated with linear dyamics A, B and quadratic cost Q, R\n",
    "\n",
    "NOTE: Current version only works for cartpole because I hardcoded a couple of numbers for now\n",
    "'''\n",
    "\n",
    "def lqr_infinite_horizon(A, B, Q, R):\n",
    "    nA = A.shape[0]\n",
    "\n",
    "    if len(B.shape) == 1:\n",
    "        nB = 1\n",
    "    else:\n",
    "        nB = B.shape[1]\n",
    "\n",
    "    P_current = np.zeros([nA, nA])\n",
    "\n",
    "    P_new = np.eye(nA)\n",
    "\n",
    "    K_current = np.zeros([nB, nA])\n",
    "\n",
    "    K_new= np.triu(np.tril(np.ones([nB,nA]),0),0)\n",
    "\n",
    "    while np.linalg.norm(K_new - K_current, 2) > 1E-4:\n",
    "        P_current = P_new\n",
    "      \n",
    "        K_current = K_new\n",
    "        \n",
    "        K_new = -np.linalg.inv(R + np.dot(np.dot( np.transpose(B), \n",
    "                                                  P_current), \n",
    "                                                  B)) * np.dot(np.dot( np.transpose(B), \n",
    "                                                                       P_current), \n",
    "                                                                       A)\n",
    "\n",
    "        P_new = Q + np.dot(np.dot( np.transpose(K_new), \n",
    "                                   R), \n",
    "                                   K_new) + np.dot(np.dot( np.transpose(A + np.dot(B.reshape(4,1), K_new)),\n",
    "                                                           P_current),\n",
    "                                                           (A + np.dot(B.reshape(4,1), K_new.reshape(1,4)))\n",
    "                          )\n",
    "        \n",
    "    return K_new, P_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_ref = np.array([0, np.pi, 0, 0])\n",
    "A, B, c = linearize_cartpole(x_ref, 0, 0.1, 0.1)\n",
    "Q = np.eye(4)\n",
    "R = np.eye(1)\n",
    "\n",
    "K_inf, P_inf = lqr_infinite_horizon(A, B, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basic implementation of a feed forward neural network with a single hidden layer. \n",
    "\n",
    "Takes input, a 1-d parameter vector, an activation function, and numbers of neurons at each layer. The parameter \n",
    "vector should be encoded as [W1.flatten(); B1; W2.flatten(); B2] where the Ws and Bs are the matrix weights and \n",
    "offset vectors. Activation function for the output layer is assumed to be linear.\n",
    "\n",
    "'''\n",
    "\n",
    "def simpleFeedForward(input, params, n_in, n_hidden, n_out, activation=np.tanh):\n",
    "    \n",
    "    ## Reshape our parameters to be used to calculate the output\n",
    "    \n",
    "    w1 = params[0 : n_in*n_hidden].reshape(n_hidden, n_in)\n",
    "    b1 = params[n_in*n_hidden : n_in*n_hidden + n_hidden]\n",
    "    w2 = params[n_in*n_hidden + n_hidden : n_in*n_hidden + n_hidden + n_hidden * n_out].reshape(n_out, n_hidden)\n",
    "    b2 = params[n_in*n_hidden + n_hidden + n_hidden * n_out:]\n",
    "    \n",
    "    lin_midstep = np.dot(w1, input) + b1\n",
    "    if activation == None:\n",
    "        midstep = lin_midstep\n",
    "    else: \n",
    "        midstep = activation(lin_midstep)\n",
    "    \n",
    "    output = np.dot(w2, midstep) + b2\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to evaluate the total penalty associated with a parameter vector theta for our simple feed forward network.\n",
    "\n",
    "Simulates cartpole with the controller given by theta and computes the sum of costs, which are assumed to be a \n",
    "quadratic form of the distance from the current position x to the target position.\n",
    "\n",
    "'''\n",
    "\n",
    "def evaluate_theta(start, target, T, Q, params, n_in, n_hidden, n_out):\n",
    "\n",
    "    dt = 0.1\n",
    "    \n",
    "    u = np.zeros(T)\n",
    "    x = np.zeros([n_in, T+1])\n",
    "    x[:, 0] = start\n",
    "    for t in range(T):\n",
    "        u[t] = simpleFeedForward(x[:, t], params, n_in, n_hidden, n_out)\n",
    "        x[:, t+1] = sim_cartpole(x[:,t], u[t], dt)\n",
    "    \n",
    "    x_diff = x - np.transpose(np.tile(target, (x.shape[1],1)))\n",
    "    penalty =  np.sum(np.diag(np.dot(np.transpose(x_diff), np.dot(Q, x_diff))))\n",
    "    \n",
    "    return penalty, x, u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:13: RuntimeWarning: overflow encountered in power\n",
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:14: RuntimeWarning: overflow encountered in power\n",
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: RuntimeWarning: invalid value encountered in sin\n",
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:11: RuntimeWarning: invalid value encountered in cos\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Some code to try to run policy gradient. Does not work yet - blows up to NaNs.\n",
    "\n",
    "'''\n",
    "\n",
    "##\n",
    "## Initialize our variables\n",
    "##\n",
    "rng = np.random.RandomState(1234)\n",
    "input = np.array([0, np.pi - np.pi/10, 0, 0])\n",
    "n_in = 4\n",
    "n_hidden = 20\n",
    "n_out = 1\n",
    "\n",
    "W1 = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = -np.sqrt(6. / (n_in + n_hidden)), \n",
    "                    high = np.sqrt(6. / (n_in + n_hidden)),\n",
    "                    size = (n_in, n_hidden)\n",
    "                ))\n",
    "\n",
    "W2 = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = -np.sqrt(6. / (n_out + n_hidden)), \n",
    "                    high = np.sqrt(6. / (n_out + n_hidden)),\n",
    "                    size = (n_hidden, n_out)\n",
    "                ))\n",
    "\n",
    "B1 = np.zeros([n_hidden,1])\n",
    "B2 = np.zeros([n_out, 1])\n",
    "\n",
    "## Cost function penalizes equal deviation from target coordinates\n",
    "target = np.array([0, np.pi, 0, 0])\n",
    "Q = np.eye(n_in)\n",
    "\n",
    "def penalty(x):\n",
    "    if len(x.shape) == 1:\n",
    "        return np.dot(x-target,np.dot(Q, x-target))\n",
    "    else:\n",
    "        x_diff = x - np.transpose(np.tile(target, (x.shape[1],1)))\n",
    "        return np.sum(np.diag(np.dot(np.transpose(x_diff), np.dot(Q, x_diff))))\n",
    "    \n",
    "\n",
    "\n",
    "params = np.append(np.append(np.append(W1.flatten(), B1), W2.flatten()), B2)\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "penalties = np.zeros(max_iter)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    ## Generate trajectory to evaluate our policy\n",
    "\n",
    "    start = np.array([0, np.pi - np.pi/10, 0, 0])\n",
    "    \n",
    "    penalties[i], x, u = evaluate_theta(start, target, 500, Q, params, n_in, n_hidden, n_out)\n",
    "        \n",
    "    ## Perform stochastic gradient descent\n",
    "    \n",
    "    # Choose random parameter to update\n",
    "    direction = np.random.randint(0, len(params))\n",
    "    \n",
    "    unitv = np.zeros(len(params))\n",
    "    unitv[direction] = epsilon\n",
    "    \n",
    "    # Calculate the penalty in that direction\n",
    "    new_penalty, x, u = evaluate_theta(start, target, 500, Q, params + unitv, n_in, n_hidden, n_out)\n",
    "    \n",
    "    partial_x = unitv\n",
    "    partial_x[direction] = (new_penalty - penalties[i])/epsilon\n",
    "    \n",
    "    ## Update the parameter vector\n",
    "    params = params - learning_rate * partial_x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of hidden layer class. Modified from here: http://deeplearning.net/tutorial/mlp.html#mlp.\n",
    "\n",
    "Note: I changed feeding forward to a function because I couldn't figure out how to update the internal state 'input'\n",
    "    when iterating to generate paths. May be better to switch back to that in the long run to make more layers\n",
    "    easier to link together?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, n_in, n_out, W=None, b=None, activation=T.tanh):\n",
    "        self.input = input\n",
    "        \n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = -np.sqrt(6. / (n_in + n_out)), \n",
    "                    high = np.sqrt(6. / (n_in + n_out)),\n",
    "                    size = (n_in, n_out)\n",
    "                ),\n",
    "                dtype = theano.config.floatX)\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "                \n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype = theano.config.floatX)\n",
    "            b = theano.shared(value = b_values, name='b', borrow=True)\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "    def feed_forward(self, input):\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Class wrapper for our basic feed forward network in Theano. \n",
    "\n",
    "Modified from here: http://deeplearning.net/tutorial/mlp.html#mlp.\n",
    "\n",
    "Note: similar to above, modified to make feed-forward a function in the main version\n",
    "'''\n",
    "\n",
    "class SingleLayerNet(object):\n",
    "    \n",
    "    def __init__(self, rng, n_in, n_hidden, n_out):\n",
    "        \n",
    "        dt = 0.1  ## Delete?\n",
    "        \n",
    "        num_steps = 10  ## Delete?\n",
    "        \n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng = rng,\n",
    "            n_in = n_in, \n",
    "            n_out = n_hidden, \n",
    "            activation = T.tanh\n",
    "        )\n",
    "        \n",
    "        self.outputLayer = HiddenLayer(\n",
    "            rng = rng,\n",
    "            n_in = n_hidden,\n",
    "            n_out = n_out,\n",
    "            activation = None\n",
    "        )\n",
    "             \n",
    "        \n",
    "        ## L1 and L2 regularization. Not used for now.\n",
    "        self.L1 = ( abs(self.hiddenLayer.W).sum() + abs(self.outputLayer.W).sum() )\n",
    "        \n",
    "        self.L2_sqr = ( (self.hiddenLayer.W ** 2).sum() + (self.outputLayer.W ** 2).sum())\n",
    "        \n",
    "        ## Don't think notion of errors makes sense\n",
    "        #self.errors = self.logRegressionLayer.errors\n",
    "        \n",
    "        self.params = self.hiddenLayer.params + self.outputLayer.params\n",
    "        \n",
    "        \n",
    "    def feed_forward(self, input):\n",
    "        return self.outputLayer.feed_forward(self.hiddenLayer.feed_forward(input))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define a function symbollically that computes a trajectory for cartpole from x0 according to a given policy\n",
    "'''\n",
    "\n",
    "C = T.scalar()\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "policy = SingleLayerNet(rng, 4, 10, 1)\n",
    "\n",
    "## Define variables\n",
    "mc = 10\n",
    "mp = 1\n",
    "l = 0.5\n",
    "g = 9.81\n",
    "\n",
    "## Compute the cartpole dyanmics updates\n",
    "def calc_step(x, u, dt):\n",
    "    xdot = T.concatenate( [\n",
    "                x[2:4], \n",
    "                (u + mp*T.sin(x[1])*(l*x[3]**2 + g*T.cos(x[1])))/(mc + mp*T.sin(x[1])**2),\n",
    "                (-u*T.cos(x[1]) -(mp*l*x[3]**2) * T.cos(x[1]) * T.sin(x[1]) - \n",
    "                     (mc+mp)*g* T.sin(x[1])) / (l * (mc + mp * T.sin(x[1]) ** 2))\n",
    "            ])\n",
    "    \n",
    "    return x + dt * xdot\n",
    "\n",
    "## Symbolically compute the trajectory associated with the policy encoded in our network\n",
    "gen_traj, traj_update = theano.scan(\n",
    "    lambda x, u, cost, dt, Q, x_t: [\n",
    "        calc_step(x,u,dt), \n",
    "        policy.feed_forward(calc_step(x,u,dt)), \n",
    "        T.dot(T.dot(x - x_t, Q), x-x_t)\n",
    "    ],\n",
    "    outputs_info = [x, policy.feed_forward(x), T.ones_like(C)],\n",
    "    non_sequences = [dt, Q, x_t],\n",
    "    n_steps = num_steps\n",
    ")\n",
    "\n",
    "## Function we will use to simulate\n",
    "sim_cartpole_T = theano.function([x, dt, C, Q, x_t, num_steps], gen_traj, updates=traj_update) ## Can we remove C??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  -2.47799548 -113.49662655   -0.35523164  -18.69905353]\n",
      " [  -2.51351865 -115.3665319    -1.067611    -16.62361342]\n",
      " [  -2.62027975 -117.02889324   -2.0287525   -16.3569875 ]\n",
      " [  -2.823155   -118.66459199   -1.1774809   -16.55228305]\n",
      " [  -2.94090309 -120.3198203    -0.27601218  -19.20026729]\n",
      " [  -2.96850431 -122.23984703   -1.72444398  -15.90647545]\n",
      " [  -3.1409487  -123.83049457   -2.0547737   -15.99450836]\n",
      " [  -3.34642607 -125.42994541   -0.95323955  -17.31783526]\n",
      " [  -3.44175003 -127.16172893   -0.59309686  -18.47300343]\n",
      " [  -3.50105972 -129.00902928   -2.15443399  -16.28914898]]\n",
      "Subtensor{int64::}.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Example of simulating the cartpole\n",
    "'''\n",
    "\n",
    "x_traj, u_traj, c_traj = sim_cartpole_T(x0, 0.1, 1, np.eye(4), np.array([0,np.pi, 0, 0]), 100)\n",
    "\n",
    "print x_traj[-10:]\n",
    "\n",
    "print gen_traj[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Next steps: this will allow us to compute the cost of a trajectory for our starting vals, target vals, and parameters.\n",
    "\n",
    "Then it will symbolically calculate the gradient, allowing us to do gradient descent.\n",
    "'''\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "x_t = T.vector('x_t')\n",
    "num_steps = T.iscalar('num_steps')\n",
    "Q = T.matrix('Q')\n",
    "\n",
    "## Compute the cost associated with a given trajectory\n",
    "\n",
    "total_cost = T.sum(gen_traj[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
