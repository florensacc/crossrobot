{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "Simulates cartpole starting at x0 with action u -- ported from CS287 Matlab code\n",
    "'''\n",
    "\n",
    "def sim_cartpole(x0, u, dt):\n",
    "    \n",
    "    def dynamics(x, u):\n",
    "        mc = 10\n",
    "        mp = 1\n",
    "        l = 0.5\n",
    "        g = 9.81\n",
    "        T = 0.25\n",
    "        s = np.sin(x[1])\n",
    "        c = np.cos(x[1])\n",
    "        \n",
    "        xddot = (u + np.multiply(mp*s, l*np.power(x[3],2) + g*c))/(mc + mp*np.power(s,2))\n",
    "        tddot = (-u*c - np.multiply(np.multiply(mp*l*np.power(x[3],2), c),s) - \n",
    "                 np.multiply((mc+mp)*g,s)) / (l * (mc + np.multiply(mp, np.power(s,2))))\n",
    "        xdot = x[2:4]\n",
    "        xdot = np.append(xdot, xddot)\n",
    "        xdot = np.append(xdot, tddot)\n",
    "        \n",
    "        return xdot\n",
    "    \n",
    "    DT = 0.1\n",
    "    t = 0\n",
    "    while t < dt:\n",
    "        current_dt = min(DT, dt-t)\n",
    "        x0 = x0 + current_dt * dynamics(x0, u)\n",
    "        t = t + current_dt\n",
    "    \n",
    "    return x0\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Linearizes the dynamics of cartpole around a reference point for use in an LQR controler\n",
    "'''\n",
    "\n",
    "def linearize_cartpole(x_ref, u_ref, dt, eps):\n",
    "    A = np.zeros([4,4])\n",
    "\n",
    "    for i in range(4):\n",
    "        increment = np.zeros([4,])\n",
    "        increment[i] = eps\n",
    "        A[:,i] = (sim_cartpole(x_ref + increment, u_ref, dt) - \n",
    "                  sim_cartpole(x_ref, u_ref, dt)) / (eps)\n",
    "    \n",
    "    B = (sim_cartpole(x_ref, u_ref + eps, dt) - sim_cartpole(x_ref, u_ref, dt)) / (eps)\n",
    "    \n",
    "    c = x_ref\n",
    "    \n",
    "    return A, B, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Computes the LQR infinte horizon controller associated with linear dyamics A, B and quadratic cost Q, R\n",
    "\n",
    "NOTE: Current version only works for cartpole because I hardcoded a couple of numbers for now\n",
    "'''\n",
    "\n",
    "def lqr_infinite_horizon(A, B, Q, R):\n",
    "    nA = A.shape[0]\n",
    "\n",
    "    if len(B.shape) == 1:\n",
    "        nB = 1\n",
    "    else:\n",
    "        nB = B.shape[1]\n",
    "\n",
    "    P_current = np.zeros([nA, nA])\n",
    "\n",
    "    P_new = np.eye(nA)\n",
    "\n",
    "    K_current = np.zeros([nB, nA])\n",
    "\n",
    "    K_new= np.triu(np.tril(np.ones([nB,nA]),0),0)\n",
    "\n",
    "    while np.linalg.norm(K_new - K_current, 2) > 1E-4:\n",
    "        P_current = P_new\n",
    "      \n",
    "        K_current = K_new\n",
    "        \n",
    "        K_new = -np.linalg.inv(R + np.dot(np.dot( np.transpose(B), \n",
    "                                                  P_current), \n",
    "                                                  B)) * np.dot(np.dot( np.transpose(B), \n",
    "                                                                       P_current), \n",
    "                                                                       A)\n",
    "\n",
    "        P_new = Q + np.dot(np.dot( np.transpose(K_new), \n",
    "                                   R), \n",
    "                                   K_new) + np.dot(np.dot( np.transpose(A + np.dot(B.reshape(4,1), K_new)),\n",
    "                                                           P_current),\n",
    "                                                           (A + np.dot(B.reshape(4,1), K_new.reshape(1,4)))\n",
    "                          )\n",
    "        \n",
    "    return K_new, P_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_ref = np.array([0, np.pi, 0, 0])\n",
    "A, B, c = linearize_cartpole(x_ref, 0, 0.1, 0.1)\n",
    "Q = np.eye(4)\n",
    "R = np.eye(1)\n",
    "\n",
    "K_inf, P_inf = lqr_infinite_horizon(A, B, Q, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basic implementation of a feed forward neural network with a single hidden layer. \n",
    "\n",
    "Takes input, a 1-d parameter vector, an activation function, and numbers of neurons at each layer. The parameter \n",
    "vector should be encoded as [W1.flatten(); B1; W2.flatten(); B2] where the Ws and Bs are the matrix weights and \n",
    "offset vectors. Activation function for the output layer is assumed to be linear.\n",
    "\n",
    "'''\n",
    "\n",
    "def simpleFeedForward(input, params, n_in, n_hidden, n_out, activation=np.tanh):\n",
    "    \n",
    "    ## Reshape our parameters to be used to calculate the output\n",
    "    \n",
    "    w1 = params[0 : n_in*n_hidden].reshape(n_hidden, n_in)\n",
    "    b1 = params[n_in*n_hidden : n_in*n_hidden + n_hidden]\n",
    "    w2 = params[n_in*n_hidden + n_hidden : n_in*n_hidden + n_hidden + n_hidden * n_out].reshape(n_out, n_hidden)\n",
    "    b2 = params[n_in*n_hidden + n_hidden + n_hidden * n_out:]\n",
    "    \n",
    "    lin_midstep = np.dot(w1, input) + b1\n",
    "    if activation == None:\n",
    "        midstep = lin_midstep\n",
    "    else: \n",
    "        midstep = activation(lin_midstep)\n",
    "    \n",
    "    output = np.dot(w2, midstep) + b2\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to evaluate the total penalty associated with a parameter vector theta for our simple feed forward network.\n",
    "\n",
    "Simulates cartpole with the controller given by theta and computes the sum of costs, which are assumed to be a \n",
    "quadratic form of the distance from the current position x to the target position.\n",
    "\n",
    "'''\n",
    "\n",
    "def evaluate_theta(start, target, T, Q, params, n_in, n_hidden, n_out):\n",
    "\n",
    "    dt = 0.1\n",
    "    \n",
    "    u = np.zeros(T)\n",
    "    x = np.zeros([n_in, T+1])\n",
    "    x[:, 0] = start\n",
    "    for t in range(T):\n",
    "        u[t] = simpleFeedForward(x[:, t], params, n_in, n_hidden, n_out)\n",
    "        x[:, t+1] = sim_cartpole(x[:,t], u[t], dt)\n",
    "    \n",
    "    x_diff = x - np.transpose(np.tile(target, (x.shape[1],1)))\n",
    "    penalty =  np.sum(np.diag(np.dot(np.transpose(x_diff), np.dot(Q, x_diff))))\n",
    "    \n",
    "    return penalty, x, u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:13: RuntimeWarning: overflow encountered in power\n",
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:14: RuntimeWarning: overflow encountered in power\n",
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: RuntimeWarning: invalid value encountered in sin\n",
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:11: RuntimeWarning: invalid value encountered in cos\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Some code to try to run policy gradient. Does not work yet - blows up to NaNs.\n",
    "\n",
    "'''\n",
    "\n",
    "##\n",
    "## Initialize our variables\n",
    "##\n",
    "rng = np.random.RandomState(1234)\n",
    "input = np.array([0, np.pi - np.pi/10, 0, 0])\n",
    "n_in = 4\n",
    "n_hidden = 20\n",
    "n_out = 1\n",
    "\n",
    "W1 = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = -np.sqrt(6. / (n_in + n_hidden)), \n",
    "                    high = np.sqrt(6. / (n_in + n_hidden)),\n",
    "                    size = (n_in, n_hidden)\n",
    "                ))\n",
    "\n",
    "W2 = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = -np.sqrt(6. / (n_out + n_hidden)), \n",
    "                    high = np.sqrt(6. / (n_out + n_hidden)),\n",
    "                    size = (n_hidden, n_out)\n",
    "                ))\n",
    "\n",
    "B1 = np.zeros([n_hidden,1])\n",
    "B2 = np.zeros([n_out, 1])\n",
    "\n",
    "## Cost function penalizes equal deviation from target coordinates\n",
    "target = np.array([0, np.pi, 0, 0])\n",
    "Q = np.eye(n_in)\n",
    "\n",
    "def penalty(x):\n",
    "    if len(x.shape) == 1:\n",
    "        return np.dot(x-target,np.dot(Q, x-target))\n",
    "    else:\n",
    "        x_diff = x - np.transpose(np.tile(target, (x.shape[1],1)))\n",
    "        return np.sum(np.diag(np.dot(np.transpose(x_diff), np.dot(Q, x_diff))))\n",
    "    \n",
    "\n",
    "\n",
    "params = np.append(np.append(np.append(W1.flatten(), B1), W2.flatten()), B2)\n",
    "\n",
    "max_iter = 100\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "penalties = np.zeros(max_iter)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    ## Generate trajectory to evaluate our policy\n",
    "\n",
    "    start = np.array([0, np.pi - np.pi/10, 0, 0])\n",
    "    \n",
    "    penalties[i], x, u = evaluate_theta(start, target, 500, Q, params, n_in, n_hidden, n_out)\n",
    "        \n",
    "    ## Perform stochastic gradient descent\n",
    "    \n",
    "    # Choose random parameter to update\n",
    "    direction = np.random.randint(0, len(params))\n",
    "    \n",
    "    unitv = np.zeros(len(params))\n",
    "    unitv[direction] = epsilon\n",
    "    \n",
    "    # Calculate the penalty in that direction\n",
    "    new_penalty, x, u = evaluate_theta(start, target, 500, Q, params + unitv, n_in, n_hidden, n_out)\n",
    "    \n",
    "    partial_x = unitv\n",
    "    partial_x[direction] = (new_penalty - penalties[i])/epsilon\n",
    "    \n",
    "    ## Update the parameter vector\n",
    "    params = params - learning_rate * partial_x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
