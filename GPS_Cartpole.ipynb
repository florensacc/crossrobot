{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "theano.config.exception_verbosity='high'\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    \"\"\"\n",
    "    Implementation of hidden layer class. Source: http://deeplearning.net/tutorial/mlp.html#mlp.\n",
    "\n",
    "     :type rng: numpy.random.RandomState\n",
    "    :param rng: a random number generator used to initialize weights\n",
    "\n",
    "    :type input: theano.tensor.dmatrix\n",
    "    :param input: a symbolic tensor of shape (n_examples, n_in)\n",
    "\n",
    "    :type n_in: int\n",
    "    :param n_in: dimensionality of input\n",
    "\n",
    "    :type n_out: int\n",
    "    :param n_out: number of hidden units\n",
    "\n",
    "    :type activation: theano.Op or function\n",
    "    :param activation: Non linearity to be applied in the hidden layer\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None, activation=T.tanh):\n",
    "        self.input = input\n",
    "        \n",
    "        if W is None:\n",
    "            W_values = np.asarray(\n",
    "                rng.uniform(\n",
    "                    low = -np.sqrt(6. / (n_in + n_out)), \n",
    "                    high = np.sqrt(6. / (n_in + n_out)), \n",
    "                    size = (n_in, n_out)\n",
    "                ),\n",
    "                dtype = theano.config.floatX)\n",
    "            \n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "                \n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "            \n",
    "        if b is None:\n",
    "            b_values = np.zeros((n_out,), dtype = theano.config.floatX)\n",
    "            b = theano.shared(value = b_values, name='b', borrow=True)\n",
    "        \n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        \n",
    "        self.params = [self.W, self.b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Simulates cartpole starting at x0 with action u -- ported from CS287 Matlab code\n",
    "'''\n",
    "\n",
    "def sim_cartpole(x0, u, dt):\n",
    "    \n",
    "    def dynamics(x, u):\n",
    "        mc = 10\n",
    "        mp = 1\n",
    "        l = 0.5\n",
    "        g = 9.81\n",
    "        T = 0.25\n",
    "        s = np.sin(x[1])\n",
    "        c = np.cos(x[1])\n",
    "        \n",
    "        xddot = (u + np.multiply(mp*s, l*np.power(x[3],2) + g*c))/(mc + mp*np.power(s,2))\n",
    "        tddot = (-u*c - np.multiply(np.multiply(mp*l*np.power(x[3],2), c),s) - \n",
    "                 np.multiply((mc+mp)*g,s)) / (l * (mc + np.multiply(mp, np.power(s,2))))\n",
    "        xdot = x[2:4]\n",
    "        xdot = np.append(xdot, xddot)\n",
    "        xdot = np.append(xdot, tddot)\n",
    "        \n",
    "        return xdot\n",
    "    \n",
    "    DT = 0.1\n",
    "    t = 0\n",
    "    while t < dt:\n",
    "        current_dt = min(DT, dt-t)\n",
    "        x0 = x0 + current_dt * dynamics(x0, u)\n",
    "        t = t + current_dt\n",
    "    \n",
    "    return x0\n",
    "    \n",
    "'''\n",
    "Linearizes the dynamics of cartpole around a reference point for use in an LQR controler\n",
    "'''\n",
    "\n",
    "def linearize_cartpole(x_ref, u_ref, dt, eps):\n",
    "    A = np.zeros([4,4])\n",
    "\n",
    "    for i in range(4):\n",
    "        increment = np.zeros([4,])\n",
    "        increment[i] = eps\n",
    "        A[:,i] = (sim_cartpole(x_ref + increment, u_ref, dt) - \n",
    "                  sim_cartpole(x_ref, u_ref, dt)) / (eps)\n",
    "    \n",
    "    B = (sim_cartpole(x_ref, u_ref + eps, dt) - sim_cartpole(x_ref, u_ref, dt)) / (eps)\n",
    "    \n",
    "    c = x_ref\n",
    "    \n",
    "    return A, B, c\n",
    "\n",
    "'''\n",
    "Computes the LQR infinte horizon controller associated with linear dyamics A, B and quadratic cost Q, R\n",
    "\n",
    "NOTE: Current version only works for cartpole because I hardcoded a couple of numbers for now\n",
    "'''\n",
    "\n",
    "def lqr_infinite_horizon(A, B, Q, R):\n",
    "    nA = A.shape[0]\n",
    "\n",
    "    if len(B.shape) == 1:\n",
    "        nB = 1\n",
    "    else:\n",
    "        nB = B.shape[1]\n",
    "\n",
    "    P_current = np.zeros([nA, nA])\n",
    "\n",
    "    P_new = np.eye(nA)\n",
    "\n",
    "    K_current = np.zeros([nB, nA])\n",
    "\n",
    "    K_new= np.triu(np.tril(np.ones([nB,nA]),0),0)\n",
    "\n",
    "    while np.linalg.norm(K_new - K_current, 2) > 1E-4:\n",
    "        P_current = P_new\n",
    "      \n",
    "        K_current = K_new\n",
    "        \n",
    "        Quu = R + np.dot(np.dot( np.transpose(B), P_current), B)\n",
    "        \n",
    "        K_new = -np.linalg.inv(Quu) * np.dot(np.dot( np.transpose(B), P_current), A)\n",
    "    \n",
    "        P_new = Q + np.dot(np.dot( np.transpose(K_new), \n",
    "                                   R), \n",
    "                                   K_new) + np.dot(np.dot( np.transpose(A + np.dot(B.reshape(4,1), K_new)),\n",
    "                                                           P_current),\n",
    "                                                           (A + np.dot(B.reshape(4,1), K_new.reshape(1,4)))\n",
    "                          )\n",
    "        \n",
    "    return K_new, P_new, Quu\n",
    "\n",
    "'''\n",
    "Generate LQR trajectory for solving our simple cartpole problem\n",
    "'''\n",
    "x_ref = np.array([0, np.pi, 0, 0])\n",
    "u_ref = 0.\n",
    "A, B, c = linearize_cartpole(x_ref, u_ref, 0.1, 0.1)\n",
    "Q = np.eye(4)\n",
    "R = np.eye(1)\n",
    "dt = 0.1\n",
    "x_init = np.array([0, np.pi - np.pi/10, 0, 0])\n",
    "\n",
    "K_inf, P_inf, Quu = lqr_infinite_horizon(A, B, Q, R)\n",
    "\n",
    "x_lqr = np.zeros([4,500])\n",
    "u_lqr = np.zeros([1,500])\n",
    "\n",
    "x_lqr[:,0] = np.array([0, np.pi - np.pi/10, 0, 0])\n",
    "u_lqr[:,0] = np.dot(K_inf, (x_lqr[:,0] - x_ref)) + u_ref\n",
    "\n",
    "for i in range(499):\n",
    "    x_lqr[:,i+1] = sim_cartpole(x_lqr[:,i], u_lqr[:,i], dt)\n",
    "    u_lqr[:,i+1] = np.dot(K_inf, (x_lqr[:,i] - x_ref) ) + u_ref\n",
    "\n",
    "    \n",
    "'''\n",
    "Function to generate samples from the guidance trajectory\n",
    "'''\n",
    "def gen_traj_guidance(x_init, x_ref, u_ref, K, variance, traj_size, dt):\n",
    "    xs = len(x_ref)\n",
    "    \n",
    "    if type(u_ref) == float:\n",
    "        us = 1\n",
    "    else:\n",
    "        us = len(u_ref)\n",
    "    \n",
    "    x_traj = np.zeros([xs, traj_size])\n",
    "    u_traj = np.zeros([us, traj_size])\n",
    "    \n",
    "    x_traj[:,0] = x_init\n",
    "    u_traj[:,0] = np.random.multivariate_normal(np.dot(K, (x_traj[:,0] - x_ref) ) + u_ref, variance)\n",
    "    \n",
    "    for t in range(traj_size-1):\n",
    "        x_traj[:,t+1] = sim_cartpole(x_traj[:,t], u_traj[:,t], dt)\n",
    "        u_mean = np.dot(K, (x_traj[:,t] - x_ref) ) + u_ref\n",
    "        u_traj[:,t+1] = np.random.multivariate_normal(u_mean, variance)\n",
    "    \n",
    "    return x_traj, u_traj\n",
    "\n",
    "x_init_options = [\n",
    "    np.array([0, np.pi - np.pi/4, 0, 0]),\n",
    "    np.array([10, np.pi - np.pi/4, 0, 0]),\n",
    "    np.array([-10, np.pi - np.pi/4, 0, 0]),\n",
    "    np.array([0, np.pi + np.pi/4, 0, 0]),\n",
    "    np.array([10, np.pi + np.pi/4, 0, 0]),\n",
    "    np.array([-10, np.pi + np.pi/4, 0, 0]),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleNN(object):\n",
    "    ''' Simple Neural Net class\n",
    "    \n",
    "    Single hidden layer net with one layer of hidden units and nonlinear activations. Top layer is linear\n",
    "    Source: http://deeplearning.net/tutorial/mlp.html#mlp\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, rng, input, n_in, n_hidden, n_out):\n",
    "        \"\"\"Initialize the parameters for the multilayer perceptron\n",
    "\n",
    "        :type rng: numpy.random.RandomState\n",
    "        :param rng: a random number generator used to initialize weights\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: symbolic variable that describes the input (one minibatch)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: number of input units\n",
    "        \n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of output units\n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        self.hiddenLayer = HiddenLayer(\n",
    "            rng = rng,\n",
    "            input=input,\n",
    "            n_in = n_in, \n",
    "            n_out = n_hidden, \n",
    "            activation = T.tanh\n",
    "        )\n",
    "        \n",
    "        self.outputLayer = HiddenLayer(\n",
    "            rng = rng,\n",
    "            input = self.hiddenLayer.output,\n",
    "            n_in = n_hidden,\n",
    "            n_out = n_out,\n",
    "            activation = None\n",
    "        )\n",
    "             \n",
    "        \n",
    "        ## L1 and L2 regularization\n",
    "        self.L1 = ( abs(self.hiddenLayer.W).sum() + abs(self.outputLayer.W).sum() )\n",
    "        \n",
    "        self.L2_sqr = ( (self.outputLayer.W ** 2).sum() + (self.hiddenLayer.W ** 2).sum())\n",
    "        \n",
    "        # neg log likelihood is given by that of the output\n",
    "        #self.negative_log_likelihood = ( self.logRegressionLayer.negative_log_likelihood )\n",
    "        #self.errors = self.logRegressionLayer.errors\n",
    "        \n",
    "        self.params = self.hiddenLayer.params + self.outputLayer.params\n",
    "        \n",
    "        self.input = input\n",
    "        \n",
    "        self.output = self.outputLayer.output\n",
    "        \n",
    "    def meanSqErr(self, u):\n",
    "        if u.ndim != self.output.ndim:\n",
    "            raise TypeError(\n",
    "                'u should have the same shape as self.output',\n",
    "                ('u', u.type, 'self.output', self.output.type)\n",
    "            )\n",
    "        return T.mean( (self.output - u)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_NN(learning_rate=0.01, L1_reg=0.0, L2_reg=0.0001, n_epochs=1000, batch_size=20, n_hidden=20, \n",
    "             LQR_controller=K_inf, LQR_start=x_init, LQR_var=Quu, num_traj=10):\n",
    "    \n",
    "    #########################\n",
    "    # GENERATE TRAJECTORIES #\n",
    "    #########################\n",
    "    \n",
    "    x_traj_list = []\n",
    "    u_traj_list = []\n",
    "    # Generate num_traj sample trajectories from our LQR policy for each of training, validation, and test\n",
    "    if type(LQR_start) == list:\n",
    "        n_guidance = len(LQR_start)\n",
    "        for i in range(3): # training, test, validiation\n",
    "            for j in range(n_guidance): # starting positions\n",
    "                for k in range(num_traj): # generate this many trajectories\n",
    "                    x_traj1, u_traj1 = gen_traj_guidance(LQR_start[j], x_ref, u_ref, LQR_controller, LQR_var, 500, dt)\n",
    "                    x_traj_list.append(x_traj1)\n",
    "                    u_traj_list.append(u_traj1)\n",
    "    else:\n",
    "        n_guidance = 1\n",
    "        for t in range(3*num_traj):\n",
    "            x_traj1, u_traj1 = gen_traj_guidance(LQR_start, x_ref, u_ref, LQR_controller, LQR_var, 500, dt)\n",
    "            x_traj_list.append(x_traj1)\n",
    "            u_traj_list.append(u_traj1)\n",
    "        \n",
    "    train_set_x = theano.shared(\n",
    "        value = np.concatenate(x_traj_list[:n_guidance*num_traj], axis=1).T, \n",
    "        name='tr_x', borrow=True)\n",
    "    train_set_u = theano.shared(\n",
    "        value = np.concatenate(u_traj_list[:n_guidance*num_traj], axis=1).T, \n",
    "        name='tr_u', borrow=True)\n",
    "    valid_set_x = theano.shared(\n",
    "        np.concatenate(x_traj_list[n_guidance*num_traj:2*n_guidance*num_traj], axis=1).T, \n",
    "        name='v_x', borrow=True)\n",
    "    valid_set_u = theano.shared(\n",
    "        np.concatenate(u_traj_list[n_guidance*num_traj:2*n_guidance*num_traj], axis=1).T, \n",
    "        name='v_u', borrow=True)\n",
    "    test_set_x = theano.shared(\n",
    "        np.concatenate(x_traj_list[2*n_guidance*num_traj:3*n_guidance*num_traj], axis=1).T, \n",
    "        name='te_x', borrow=True)\n",
    "    test_set_u = theano.shared(\n",
    "        np.concatenate(u_traj_list[n_guidance*num_traj:2*n_guidance*num_traj], axis=1).T, \n",
    "        name='te_u', borrow=True)\n",
    "    \n",
    "    #############################################\n",
    "    # SIMPLER VERSION: TRYING TO LEARN DET. LQR #\n",
    "    #############################################\n",
    "    #train_set_x = theano.shared(value = x_lqr.T, name='tr_x', borrow=True)\n",
    "    #train_set_u = theano.shared(value = u_lqr.T, name='tr_u', borrow=True)\n",
    "    #valid_set_x = theano.shared(value = x_lqr.T, name='v_x', borrow=True)\n",
    "    #valid_set_u = theano.shared(value = u_lqr.T, name='v_u', borrow=True)\n",
    "    #test_set_x = theano.shared(value = x_lqr.T, name='te_x', borrow=True)\n",
    "    #test_set_u = theano.shared(value = u_lqr.T, name='t_u', borrow=True)\n",
    "\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print '... building the model'\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as a matrix of positions\n",
    "    u = T.matrix('u')  # control inputs are a 1d matrix\n",
    "\n",
    "    rng = np.random.RandomState(1234)\n",
    "\n",
    "    # construct the MLP class\n",
    "    classifier = SimpleNN(\n",
    "        rng=rng,\n",
    "        input=x,\n",
    "        n_in=4,\n",
    "        n_hidden=n_hidden,\n",
    "        n_out=1\n",
    "    )\n",
    "\n",
    "    # define cost function\n",
    "    cost = (\n",
    "        classifier.meanSqErr(u)\n",
    "        + L1_reg * classifier.L1\n",
    "        + L2_reg * classifier.L2_sqr\n",
    "    )\n",
    "    \n",
    "    # compile a Theano function that computes the mistakes that are made by the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            u: test_set_u[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            u: valid_set_u[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # calculate gradient\n",
    "    gparams = [T.grad(cost, param) for param in classifier.params]\n",
    "    \n",
    "    # rule for parameter updates\n",
    "    updates = [\n",
    "        (param, param- learning_rate * gparam)\n",
    "        for param, gparam in zip(classifier.params, gparams)\n",
    "    ]\n",
    "    \n",
    "    # return cost and update parameters\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            u: train_set_u[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print '... training'\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 10000000  # look as this many examples regardless (10000)\n",
    "    patience_increase = 2  # wait this much longer when a new best is found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i) for i\n",
    "                                     in xrange(n_valid_batches)]\n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "                \n",
    "                if (iter + 1) % (validation_frequency * 100.) == 0:\n",
    "                    print(\n",
    "                        'epoch %i, minibatch %i/%i, validation error %f' %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            this_validation_loss\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if (\n",
    "                        this_validation_loss < best_validation_loss *\n",
    "                        improvement_threshold\n",
    "                    ):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [test_model(i) for i\n",
    "                                   in xrange(n_test_batches)]\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    if (iter +1 % (validation_frequency * 100.) == 0):\n",
    "                        print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                               'best model %f') %\n",
    "                              (epoch, minibatch_index + 1, n_train_batches,\n",
    "                               test_score))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete. Best validation score of %f '\n",
    "           'obtained at iteration %i, with test performance %f') %\n",
    "          (best_validation_loss, best_iter + 1, test_score))\n",
    "    print 'The code ran for %d epochs, with %f epochs/sec' % (\n",
    "        epoch, 1.*epoch / (end_time - start_time))\n",
    "    \n",
    "    return x, classifier\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... training\n",
      "epoch 100, minibatch 600/600, validation error 187.451360\n",
      "epoch 200, minibatch 600/600, validation error 110.169057\n",
      "epoch 300, minibatch 600/600, validation error 67.629199\n",
      "epoch 400, minibatch 600/600, validation error 35.693192\n",
      "epoch 500, minibatch 600/600, validation error 47.166132\n",
      "epoch 600, minibatch 600/600, validation error 15.588173\n",
      "epoch 700, minibatch 600/600, validation error 15.600323\n",
      "epoch 800, minibatch 600/600, validation error 14.363079\n",
      "epoch 900, minibatch 600/600, validation error 12.071353\n",
      "epoch 1000, minibatch 600/600, validation error 12.771165\n",
      "epoch 1100, minibatch 600/600, validation error 10.166929\n",
      "epoch 1200, minibatch 600/600, validation error 9.855234\n",
      "epoch 1300, minibatch 600/600, validation error 9.238201\n",
      "epoch 1400, minibatch 600/600, validation error 9.138170\n",
      "epoch 1500, minibatch 600/600, validation error 8.741061\n",
      "epoch 1600, minibatch 600/600, validation error 8.121983\n",
      "epoch 1700, minibatch 600/600, validation error 7.678898\n",
      "epoch 1800, minibatch 600/600, validation error 7.425013\n",
      "epoch 1900, minibatch 600/600, validation error 7.295546\n",
      "epoch 2000, minibatch 600/600, validation error 7.095721\n",
      "epoch 2100, minibatch 600/600, validation error 6.651353\n",
      "epoch 2200, minibatch 600/600, validation error 6.266651\n",
      "epoch 2300, minibatch 600/600, validation error 6.111607\n",
      "epoch 2400, minibatch 600/600, validation error 6.044669\n",
      "epoch 2500, minibatch 600/600, validation error 6.009271\n",
      "epoch 2600, minibatch 600/600, validation error 6.004334\n",
      "epoch 2700, minibatch 600/600, validation error 5.991415\n",
      "epoch 2800, minibatch 600/600, validation error 5.929805\n",
      "epoch 2900, minibatch 600/600, validation error 6.235158\n",
      "epoch 3000, minibatch 600/600, validation error 6.432901\n",
      "epoch 3100, minibatch 600/600, validation error 6.419899\n",
      "epoch 3200, minibatch 600/600, validation error 6.504040\n",
      "epoch 3300, minibatch 600/600, validation error 6.861558\n",
      "epoch 3400, minibatch 600/600, validation error 7.480038\n",
      "epoch 3500, minibatch 600/600, validation error 7.468375\n",
      "epoch 3600, minibatch 600/600, validation error 7.328912\n",
      "epoch 3700, minibatch 600/600, validation error 7.154010\n",
      "epoch 3800, minibatch 600/600, validation error 6.947333\n",
      "epoch 3900, minibatch 600/600, validation error 6.710376\n",
      "epoch 4000, minibatch 600/600, validation error 6.498199\n",
      "epoch 4100, minibatch 600/600, validation error 6.355819\n",
      "epoch 4200, minibatch 600/600, validation error 6.261607\n",
      "epoch 4300, minibatch 600/600, validation error 6.190730\n",
      "epoch 4400, minibatch 600/600, validation error 6.137093\n",
      "epoch 4500, minibatch 600/600, validation error 6.102553\n",
      "epoch 4600, minibatch 600/600, validation error 6.089106\n",
      "epoch 4700, minibatch 600/600, validation error 6.096470\n",
      "epoch 4800, minibatch 600/600, validation error 6.117648\n",
      "epoch 4900, minibatch 600/600, validation error 6.118747\n",
      "epoch 5000, minibatch 600/600, validation error 5.985510\n",
      "epoch 5100, minibatch 600/600, validation error 5.549853\n",
      "epoch 5200, minibatch 600/600, validation error 5.074566\n",
      "epoch 5300, minibatch 600/600, validation error 4.792721\n",
      "epoch 5400, minibatch 600/600, validation error 4.614163\n",
      "epoch 5500, minibatch 600/600, validation error 4.484513\n",
      "epoch 5600, minibatch 600/600, validation error 4.382581\n",
      "epoch 5700, minibatch 600/600, validation error 4.298954\n",
      "epoch 5800, minibatch 600/600, validation error 4.228663\n",
      "epoch 5900, minibatch 600/600, validation error 4.168611\n",
      "epoch 6000, minibatch 600/600, validation error 4.116631\n",
      "epoch 6100, minibatch 600/600, validation error 4.071114\n",
      "epoch 6200, minibatch 600/600, validation error 4.030845\n",
      "epoch 6300, minibatch 600/600, validation error 3.994900\n",
      "epoch 6400, minibatch 600/600, validation error 3.962565\n",
      "epoch 6500, minibatch 600/600, validation error 3.933278\n",
      "epoch 6600, minibatch 600/600, validation error 3.906587\n",
      "epoch 6700, minibatch 600/600, validation error 3.882124\n",
      "epoch 6800, minibatch 600/600, validation error 3.859580\n",
      "epoch 6900, minibatch 600/600, validation error 3.838701\n",
      "epoch 7000, minibatch 600/600, validation error 3.819270\n",
      "epoch 7100, minibatch 600/600, validation error 3.801109\n",
      "epoch 7200, minibatch 600/600, validation error 3.784067\n",
      "epoch 7300, minibatch 600/600, validation error 3.768020\n",
      "epoch 7400, minibatch 600/600, validation error 3.752863\n",
      "epoch 7500, minibatch 600/600, validation error 3.738511\n",
      "epoch 7600, minibatch 600/600, validation error 3.724890\n",
      "epoch 7700, minibatch 600/600, validation error 3.711939\n",
      "epoch 7800, minibatch 600/600, validation error 3.699604\n",
      "epoch 7900, minibatch 600/600, validation error 3.687838\n",
      "epoch 8000, minibatch 600/600, validation error 3.676598\n",
      "epoch 8100, minibatch 600/600, validation error 3.665845\n",
      "epoch 8200, minibatch 600/600, validation error 3.655543\n",
      "epoch 8300, minibatch 600/600, validation error 3.645658\n",
      "epoch 8400, minibatch 600/600, validation error 3.636158\n",
      "epoch 8500, minibatch 600/600, validation error 3.627014\n",
      "epoch 8600, minibatch 600/600, validation error 3.618197\n",
      "epoch 8700, minibatch 600/600, validation error 3.609682\n",
      "epoch 8800, minibatch 600/600, validation error 3.601444\n",
      "epoch 8900, minibatch 600/600, validation error 3.593461\n",
      "epoch 9000, minibatch 600/600, validation error 3.585712\n",
      "epoch 9100, minibatch 600/600, validation error 3.578178\n",
      "epoch 9200, minibatch 600/600, validation error 3.570839\n",
      "epoch 9300, minibatch 600/600, validation error 3.563680\n",
      "epoch 9400, minibatch 600/600, validation error 3.556684\n",
      "epoch 9500, minibatch 600/600, validation error 3.549838\n",
      "epoch 9600, minibatch 600/600, validation error 3.543127\n",
      "epoch 9700, minibatch 600/600, validation error 3.536540\n",
      "epoch 9800, minibatch 600/600, validation error 3.530064\n",
      "epoch 9900, minibatch 600/600, validation error 3.523688\n",
      "epoch 10000, minibatch 600/600, validation error 3.517404\n",
      "Optimization complete. Best validation score of 3.517404 obtained at iteration 6000000, with test performance 32.765746\n",
      "The code ran for 10000 epochs, with 6.938370 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "x, policy = train_NN(learning_rate=0.00005, L1_reg=0.0, L2_reg=0.0000, n_epochs=10000, batch_size=50, n_hidden=20, \n",
    "                     LQR_controller=K_inf, LQR_start=x_init_options, LQR_var=Quu, num_traj=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_forward = theano.function(\n",
    "    inputs=[x],\n",
    "    outputs=policy.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "x_init2 = np.array([3, np.pi - np.pi/4, 0, 0])\n",
    "\n",
    "x_sim = np.zeros([4,500])\n",
    "x_sim[:,0] = x_init2\n",
    "\n",
    "u_sim = np.zeros([500,])\n",
    "u_sim[0] = feed_forward(x_sim[:,0].reshape([1,4]))\n",
    "\n",
    "for t in range(499):\n",
    "    x_sim[:,t+1] = sim_cartpole(x_sim[:,t], u_sim[t], dt)\n",
    "    u_sim[t+1] = feed_forward(x_sim[:,t+1].reshape([1,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.00967144e-02,   3.14160114e+00,  -7.50992603e-04,\n",
       "        -1.21293118e-05])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sim[:,-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
